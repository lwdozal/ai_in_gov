{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert PDFs to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install PyPDF2\n",
    "# %pip install openpyxl==3.0.10\n",
    "# %pip install PyMuPDF pandas\n",
    "# %pip install pdf2image pytesseract pillow\n",
    "# %pip install nltk\n",
    "# %pip install spacy\n",
    "# %pip install transformers==4.36.2\n",
    "# %pip install --upgrade sentence-transformers\n",
    "\n",
    "# %pip install chromadb sentence-transformers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lwert\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Core\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import re #regular expression\n",
    "\n",
    "\n",
    "#OCR\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "# Tokenizers\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.max_length = 3500000\n",
    "\n",
    "# Embeddings\n",
    "import chromadb \n",
    "from chromadb.config import Settings \n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocr_on_pdfs(pdf_dir):\n",
    "    #identify the dataset\n",
    "    pdf_data = []\n",
    "\n",
    "    # Loop through all PDF files in the directory\n",
    "    for filename in os.listdir(pdf_dir):\n",
    "        if filename.endswith('.pdf'):\n",
    "            file_path = os.path.join(pdf_dir, filename)\n",
    "            print(\"working on:\", file_path)\n",
    "\n",
    "            try:\n",
    "\n",
    "                #convert file to image\n",
    "                images = convert_from_path(file_path, poppler_path=r\"C:\\poppler-24.08.0\\Library\\bin\")\n",
    "                # Extract the text from all pages\n",
    "                text = []\n",
    "                for img in images:\n",
    "                    text.append(pytesseract.image_to_string(img))\n",
    "\n",
    "                    # Get PDF metadata\n",
    "                    # Open the PDF file\n",
    "                doc = fitz.open(file_path)\n",
    "                metadata = doc.metadata\n",
    "                page_count = doc.page_count\n",
    "\n",
    "\n",
    "                # Create a dictionary to convert to dataframe\n",
    "                pdf_data.append({\n",
    "                    'filename': filename,\n",
    "                    'text': text,\n",
    "                    'page_count': page_count,\n",
    "                    'creator': metadata.get('creator'),\n",
    "                    'producer': metadata.get('producer'),\n",
    "                    'subject': metadata.get('subject'),\n",
    "                    'keywords': metadata.get('keywords'),\n",
    "                    'title': metadata.get('title'),\n",
    "                    'creation_date': metadata.get('creationDate'),\n",
    "                })\n",
    "\n",
    "                doc.close()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"OCR error on {filename}: {e}\")\n",
    "\n",
    "    return pdf_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on: pdf_data\\ARG-1472-D-2023.pdf\n",
      "working on: pdf_data\\CA-2885.pdf\n",
      "working on: pdf_data\\CA-AB2013.pdf\n",
      "working on: pdf_data\\CA-AJR6.pdf\n",
      "working on: pdf_data\\CA-SB1047.pdf\n",
      "working on: pdf_data\\CA-SB942.pdf\n",
      "working on: pdf_data\\CO-CAIA.pdf\n",
      "working on: pdf_data\\DE-H333.pdf\n",
      "working on: pdf_data\\EU-2024-1689.pdf\n",
      "OCR error on EU-2024-1689.pdf: Unable to get page count.\n",
      "Syntax Error: Couldn't find trailer dictionary\n",
      "Syntax Error: Couldn't find trailer dictionary\n",
      "Syntax Error: Couldn't read xref table\n",
      "\n",
      "working on: pdf_data\\FL-S1680.pdf\n",
      "working on: pdf_data\\IL-H4705.pdf\n",
      "working on: pdf_data\\IL-H4836.pdf\n",
      "working on: pdf_data\\IL-H4844.pdf\n",
      "working on: pdf_data\\IN-AI_task_Force.pdf\n",
      "working on: pdf_data\\MD-818.pdf\n",
      "working on: pdf_data\\MEX.pdf\n",
      "working on: pdf_data\\NH_H1688.pdf\n",
      "working on: pdf_data\\NJ-S3357.pdf\n",
      "working on: pdf_data\\NY-A8129.pdf\n",
      "working on: pdf_data\\OJ_L_202401689_EN_TXT.pdf\n",
      "working on: pdf_data\\OR-H4153.pdf\n",
      "working on: pdf_data\\PA-H49.pdf\n",
      "working on: pdf_data\\PA-HR170.pdf\n",
      "working on: pdf_data\\RI-S117.pdf\n",
      "working on: pdf_data\\TN-H2325.pdf\n",
      "working on: pdf_data\\UK.pdf\n",
      "working on: pdf_data\\UT-S149.pdf\n",
      "working on: pdf_data\\UT-SB0149.pdf\n",
      "working on: pdf_data\\VA-H747.pdf\n",
      "working on: pdf_data\\VA-S487.pdf\n",
      "working on: pdf_data\\VT-H710.pdf\n",
      "working on: pdf_data\\WA-1168.pdf\n",
      "working on: pdf_data\\WA-1170.pdf\n",
      "working on: pdf_data\\WA-5838-S2.PL.pdf\n",
      "working on: pdf_data\\WI-A664.pdf\n",
      "working on: pdf_data\\WV-H5690.pdf\n"
     ]
    }
   ],
   "source": [
    "#pull pdf data and run through ocr\n",
    "pdf_dir = \"pdf_data\"\n",
    "# pdfs = ocr_on_pdfs(pdf_dir)\n",
    "\n",
    "# Save to JSON\n",
    "# with open('legal_pdf_data.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(pdfs, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at: https://github.com/lwdozal/Chat-with-your-Research-Articles-LLM-Retrieval-Augmented-Generation/blob/main/Intermediate%20RAG%20(prefilled).ipynb\n",
    "\n",
    "Tokenize sentences into paragraphs ~5 sentences into each chunk \n",
    "Tokenize based on specific legal clauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crete function to split text based on specific legal clauses\n",
    "def split_legal_clauses(text, legal_clauses):\n",
    "    # set-up splitter to Find the clauses in the text\n",
    "    clause_splitter = re.compile(r'(?=({}))'.format('|'.join(legal_clauses)), re.IGNORECASE)\n",
    "\n",
    "    # Split based on legal clause words\n",
    "    clauses = clause_splitter.split(text)\n",
    "    \n",
    "    # Clean up chunks and stitch words to their content\n",
    "    refined_clauses = []\n",
    "    i = 0\n",
    "    while i < len(clauses):\n",
    "        if clause_splitter.match(clauses[i]):\n",
    "            # Pair clause header with its content\n",
    "            if i + 1 < len(clauses):\n",
    "                refined_clauses.append((clauses[i] + ' ' + clauses[i+1]).strip())\n",
    "                i += 2\n",
    "            else:\n",
    "                refined_clauses.append(clauses[i].strip())\n",
    "                i += 1\n",
    "        else:\n",
    "            if clauses[i].strip():\n",
    "                refined_clauses.append(clauses[i].strip())\n",
    "            i += 1\n",
    "\n",
    "    return refined_clauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up function to split text into sentences, paragraphs, and clauses\n",
    "def create_split_text(text, legal_clauses, para_size=5, overlap=2):\n",
    "    \"\"\"\n",
    "    Convert flat text into overlapping paragraphs of sentences.\n",
    "    - para_size: number of sentences per paragraph\n",
    "    - overlap: number of sentences to overlap between paragraphs\n",
    "    \"\"\"\n",
    "    #Tokenize into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    # split text based on clauses\n",
    "    clauses = split_legal_clauses(text, legal_clauses)\n",
    "\n",
    "    # split text to create overlapping paragraphs\n",
    "    paragraphs = []\n",
    "    i = 0\n",
    "    while i < len(sentences):\n",
    "        para = sentences[i:i+para_size]\n",
    "        if para:\n",
    "            paragraphs.append(' '.join(para))\n",
    "        i += para_size - overlap  # Slide forward by (para_size - overlap)\n",
    "\n",
    "    return paragraphs, sentences, clauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in Json of pdf texts\n",
    "# with open('legal_pdf_data.json', 'r', encoding='utf-8') as f:\n",
    "#     pdf_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on: ARG-1472-D-2023.pdf\n",
      "Number of words: 5432\n",
      "Working on: CA-2885.pdf\n",
      "Number of words: 8162\n",
      "Working on: CA-AB2013.pdf\n",
      "Number of words: 26023\n",
      "Working on: CA-AJR6.pdf\n",
      "Number of words: 7471\n",
      "Working on: CA-SB1047.pdf\n",
      "Number of words: 5339\n",
      "Working on: CA-SB942.pdf\n",
      "Number of words: 54031\n",
      "Working on: CO-CAIA.pdf\n",
      "Number of words: 11133\n",
      "Working on: DE-H333.pdf\n",
      "Number of words: 42107\n",
      "Working on: FL-S1680.pdf\n",
      "Number of words: 6812\n",
      "Working on: IL-H4705.pdf\n",
      "Number of words: 8586\n",
      "Working on: IL-H4836.pdf\n",
      "Number of words: 4333\n",
      "Working on: IL-H4844.pdf\n",
      "Number of words: 6078\n",
      "Working on: IN-AI_task_Force.pdf\n",
      "Number of words: 3030138\n",
      "Working on: MD-818.pdf\n",
      "Number of words: 18680\n",
      "Working on: MEX.pdf\n",
      "Number of words: 35879\n",
      "Working on: NH_H1688.pdf\n",
      "Number of words: 33008\n",
      "Working on: NJ-S3357.pdf\n",
      "Number of words: 7431\n",
      "Working on: NY-A8129.pdf\n",
      "Number of words: 7388\n",
      "Working on: OJ_L_202401689_EN_TXT.pdf\n",
      "Number of words: 16263\n",
      "Working on: OR-H4153.pdf\n",
      "Number of words: 597877\n",
      "Working on: PA-H49.pdf\n",
      "Number of words: 4712\n",
      "Working on: PA-HR170.pdf\n",
      "Number of words: 4490\n",
      "Working on: RI-S117.pdf\n",
      "Number of words: 10663\n",
      "Working on: TN-H2325.pdf\n",
      "Number of words: 8174\n",
      "Working on: UK.pdf\n",
      "Number of words: 8748\n",
      "Working on: UT-S149.pdf\n",
      "Number of words: 212399\n",
      "Working on: UT-SB0149.pdf\n",
      "Number of words: 35275\n",
      "Working on: VA-H747.pdf\n",
      "Number of words: 39725\n",
      "Working on: VA-S487.pdf\n",
      "Number of words: 20134\n",
      "Working on: VT-H710.pdf\n",
      "Number of words: 1826\n",
      "Working on: WA-1168.pdf\n",
      "Number of words: 40628\n",
      "Working on: WA-1170.pdf\n",
      "Number of words: 7027\n",
      "Working on: WA-5838-S2.PL.pdf\n",
      "Number of words: 8348\n",
      "Working on: WI-A664.pdf\n",
      "Number of words: 16291\n",
      "Working on: WV-H5690.pdf\n",
      "Number of words: 3995\n"
     ]
    }
   ],
   "source": [
    "# pdf_data\n",
    "\n",
    "# Types of legal clause headers\n",
    "LEGAL_CLAUSES = [\n",
    "    r'WHEREAS\\b',\n",
    "    r'NOW, THEREFORE\\b',\n",
    "    r'BE IT RESOLVED\\b',\n",
    "    r'IN WITNESS WHEREOF\\b',\n",
    "    r'THIS AGREEMENT\\b',\n",
    "    r'FOR THE AVOIDANCE OF DOUBT\\b',\n",
    "    r'SUBJECT TO\\b',\n",
    "    r'NOTWITHSTANDING\\b'\n",
    "]\n",
    "\n",
    "#chunk data into paragraphs and sentences for best Summarization and RAG\n",
    "for doc in pdf_data:\n",
    "    print(\"Working on:\", doc[\"filename\"])\n",
    "    print(\"Number of words:\", len(parsed.text))\n",
    "\n",
    "    text = doc['text']\n",
    "    text = \" \".join(text)\n",
    "    joined_text = text.replace(\"\\n\",\" \")\n",
    "    joined_text = joined_text.replace(\"\\n\\n\",\" \")\n",
    "    parsed = nlp(text)\n",
    "    # print(parsed.text)\n",
    "    \n",
    "    # split the text into sentences, identified clauses, and sentences\n",
    "    paragraphs, sentences, clauses = create_split_text(parsed.text, LEGAL_CLAUSES, para_size=8, overlap=3)\n",
    "\n",
    "    doc['paragraphs'] = paragraphs\n",
    "    doc['sentences'] = sentences\n",
    "    doc['claues'] = clauses\n",
    "\n",
    "with open('legal_pdf_tokenized.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(pdf_data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Text Embeddings\n",
    "\n",
    "To start creating embeddings for RAG (This might be an easier start for us, and might still hold some buzzword hype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup chromadb for chunk embeddings storage\n",
    "\n",
    "chroma_client = chromadb.Client(Settings(\n",
    "    persist_directory=\"chroma_store\",  # Save to disk\n",
    "    anonymized_telemetry=False\n",
    "))\n",
    "\n",
    "collection = chroma_client.get_or_create_collection(\"legal_ai_docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in Json of pdf text tokenization\n",
    "with open('legal_pdf_tokenized.json', 'r', encoding='utf-8') as f:\n",
    "    pdf_tokens = json.load(f)\n",
    "\n",
    "#check the json was imported correctly\n",
    "pdf_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embed the 'chunks' based on preprocessed transformer learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the sentence transformer model from huggingface and make sure it works\n",
    "# model: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "print(model.encode([\"This works!\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rag_chunks = []\n",
    "ids, texts, metadatas = [], [], []\n",
    "\n",
    "#start with paragraphs from each pdf\n",
    "for pdf in pdf_tokens:\n",
    "    filename = pdf['filename']\n",
    "    paragraphs = pdf['paragraphs']\n",
    "    print(\"working on\", filename)\n",
    "\n",
    "    #get each paragraph \n",
    "    for i, chunk in enumerate(paragraphs):\n",
    "        chunk_id = f\"{filename}_chunk{i}\"\n",
    "        ids.append(chunk_id)\n",
    "        texts.append(chunk)\n",
    "        metadatas.append({\n",
    "            \"source\": filename,\n",
    "            \"chunk_index\": i,\n",
    "            \"filepath\": f\"docs/{filename}\",   # Optional: full local or cloud path\n",
    "\n",
    "        })\n",
    "\n",
    "        rag_chunks.append({\n",
    "            \"id\": chunk_id,\n",
    "            \"text\": chunk,\n",
    "            \"metadata\": metadatas[-1]\n",
    "        })\n",
    "\n",
    "#save embeddings and metadata to the chromadb embeddings database\n",
    "embeddings = model.encode(texts).tolist()\n",
    "\n",
    "collection.add(\n",
    "    documents=texts,\n",
    "    metadatas=metadatas,\n",
    "    ids=ids,\n",
    "    embeddings=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can query chromadb for specific content in our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What happens in the case of data reusability?\"\n",
    "query_embedding = model.encode([query]).tolist()[0]\n",
    "\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_embedding],\n",
    "    n_results=5 #get the top 5 chunks\n",
    ")\n",
    "\n",
    "# Gather the top chunks + metadata\n",
    "retrieved_chunks = []\n",
    "for doc_text, meta in zip(results[\"documents\"][0], results[\"metadatas\"][0]):\n",
    "    citation_tag = f\"[{meta['source']} chunk {meta['chunk_index']}]\"\n",
    "    retrieved_chunks.append(f\"{citation_tag}\\n{doc_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CA-AB2013.pdf chunk 6]\\n(9) Whether there was any cleaning, processing, or other modification to the datasets by the developer,\\nincluding the intended purpose of those efforts in relation to the artificial intelligence system or service. (10) The time period during which the data in the datasets were collected, including a notice if the data\\ncollection is ongoing. (11) The dates the datasets were first used during the development of the artificial intelligence system or\\nservice. (12) Whether the generative artificial intelligence system or service used or continuously uses synthetic data\\ngeneration in its development. A developer may include a description of the functional need or desired\\npurpose of the synthetic data in relation to the intended purpose of the system or service. (b) A developer shall not be required to post documentation regarding the data used to train a generative artificial\\nintelligence system or service for any of the following:\\n\\n(1) A generative artificial intelligence system or service whose sole purpose is to help ensure security and\\nintegrity. For purposes of this paragraph, “security and integrity” has the same meaning as defined in\\nsubdivision (ac) of Section 1798.140, except as applied to any developer or user and not limited to businesses,\\nas defined in subdivision (d) of that section. (2) A generative artificial intelligence system or service whose sole purpose is the operation of aircraft in the\\nnational airspace.', '[CA-AB2013.pdf chunk 5]\\nFor purposes of this paragraph, the following\\ndefinitions apply:\\n\\n(A) As applied to datasets that include labels, “types of data points” means the types of labels used. (B) As applied to datasets without labeling, “types of data points” refers to the general characteristics. (5) Whether the datasets include any data protected by copyright, trademark, or patent, or whether the datasets\\nare entirely in the public domain. (6) Whether the datasets were purchased or licensed by the developer. (7) Whether the datasets include personal information, as defined in subdivision (v) of Section 1798.140.\\n\\nhttps://legiscan.com/CA/text/AB2013/id/30231 92/California-2023-AB2013-Chaptered.html 2/3\\n 3/31/25, 11:43 AM California-2023-AB2013-Chaptered\\n\\n(8) Whether the datasets include aggregate consumer information, as defined in subdivision (b) of Section\\n1798.140. (9) Whether there was any cleaning, processing, or other modification to the datasets by the developer,\\nincluding the intended purpose of those efforts in relation to the artificial intelligence system or service. (10) The time period during which the data in the datasets were collected, including a notice if the data\\ncollection is ongoing. (11) The dates the datasets were first used during the development of the artificial intelligence system or\\nservice.', '[WA-1168.pdf chunk 3]\\n(6) \"Substantially modifies\" or \"substantial modification\" means\\n\\na new version, new release, or other update to a generative\\n\\nartificial intelligence system or service that materially changes its\\nfunctionality or performance, including the results of retraining or\\nfine tuning. (7) “Synthetic data generation\" means a process in which seed\\ndata are used to create artificial data that have some of the\\nstatistical characteristics of the seed data. (8) \"Train a generative artificial intelligence system or\\nservice\" includes testing, validating, or fine tuning by the\\ndeveloper of the artificial intelligence system or service. NEW SECTION. Sec. 2. (1) On or before January 1, 2026, and\\nbefore each time thereafter that a generative artificial intelligence\\nsystem or service, or a substantial modification to a generative\\nartificial intelligence system or service, released on or after\\nJanuary 1, 2022, is made publicly available to Washingtonians for\\nuse, regardless of whether the terms of that use include\\ncompensation, the developer of the system or service shall post on\\nthe developer\\'s internet website documentation regarding the data\\nused by the developer to train the generative artificial intelligence\\nsystem or service including, but not limited to:\\n\\np. 2 HB 1168\\n MavA DOF WN rF CO WO WANA DO BF WY KH\\n\\n21\\n22\\n23\\n24\\n25\\n26\\n27\\n28\\n29\\n30\\n31\\n32\\n33\\n34\\n35\\n36\\n37\\n38\\n39\\n\\n(a) A high-level summary of the datasets used in the development\\nof the generative artificial intelligence system or service\\nincluding, but not limited to:\\n\\n(i) The sources or owners of the datasets;\\n\\n(ii) A description of how the datasets further the intended\\npurpose of the artificial intelligence system or service;\\n\\n(iii) The number of data points included in the datasets, which\\nmay be in general ranges, and with estimated figures for dynamic\\ndatasets;\\n\\n(iv) A description of the types of data points within the\\ndatasets;\\n\\n(v) Whether the datasets include any data protected by copyright,\\ntrademark, or patent, or whether the datasets are entirely in the\\npublic domain;\\n\\n(vi) Whether the datasets were purchased or licensed by the\\ndeveloper;\\n\\n(vii) Whether the datasets include personal information, as\\ndefined in RCW 19.255.005;\\n\\n(viii) Whether the datasets include aggregate consumer\\ninformation;\\n\\n(ix) Whether there was any cleaning, processing, or other\\nmodification to the datasets by the developer, including the intended\\npurpose of those efforts in relation to the artificial intelligence\\nsystem or service;\\n\\n(x) The time period during which the data in the datasets were\\ncollected, including a notice if the data collection is ongoing;\\n\\n(xi) The dates the datasets were first used during’ the\\ndevelopment of the artificial intelligence system or service; and\\n\\n(xii) Whether the generative artificial intelligence system or\\nservice used or continuously uses synthetic data generation in its\\ndevelopment. A developer may include a description of the functional\\nneed or desired purpose of the synthetic data in relation to the\\nintended purpose of the system or service.', '[OJ_L_202401689_EN_TXT.pdf chunk 198]\\nArticle 10\\n\\nData and data governance\\n\\n1. High-risk Al systems which make use of techniques involving the training of Al models with data shall be developed\\non the basis of training, validation and testing data sets that meet the quality criteria referred to in paragraphs 2 to 5\\nwhenever such data sets are used. 2. Training, validation and testing data sets shall be subject to data governance and management practices appropriate\\nfor the intended purpose of the high-risk AI system. Those practices shall concern in particular:\\n\\n(a) the relevant design choices;\\n\\n(b) data collection processes and the origin of data, and in the case of personal data, the original purpose of the data\\ncollection;\\n\\n(c) relevant data-preparation processing operations, such as annotation, labelling, cleaning, updating, enrichment and\\naggregation;\\n\\n(d) the formulation of assumptions, in particular with respect to the information that the data are supposed to measure and\\nrepresent;\\n\\n(e) an assessment of the availability, quantity and suitability of the data sets that are needed;\\n\\n(f) examination in view of possible biases that are likely to affect the health and safety of persons, have a negative impact\\non fundamental rights or lead to discrimination prohibited under Union law, especially where data outputs influence\\ninputs for future operations;\\n\\n(g) appropriate measures to detect, prevent and mitigate possible biases identified according to point (f);\\n\\n(h) the identification of relevant data gaps or shortcomings that prevent compliance with this Regulation, and how those\\ngaps and shortcomings can be addressed. 3. Training, validation and testing data sets shall be relevant, sufficiently representative, and to the best extent possible,\\nfree of errors and complete in view of the intended purpose. They shall have the appropriate statistical properties, including,\\nwhere applicable, as regards the persons or groups of persons in relation to whom the high-risk AI system is intended to be\\nused.', '[CA-AB2013.pdf chunk 3]\\n1a, or a hospital’s medical staff member. (c) “Generative artificial intelligence” means artificial intelligence that can generate derived synthetic content,\\nsuch as text, images, video, and audio, that emulates the structure and characteristics of the artificial\\nintelligence’s training data. (d) “Substantially modifies” or “substantial modification” means a new version, new release, or other update to a\\ngenerative artificial intelligence system or service that materially changes its functionality or performance,\\nincluding the results of retraining or fine tuning. (e) “Synthetic data generation” means a process in which seed data are used to create artificial data that have\\nsome of the statistical characteristics of the seed data. (f) “Train a generative artificial intelligence system or service” includes testing, validating, or fine tuning by the\\ndeveloper of the artificial intelligence system or service. 3111. On or before January 1, 2026, and before each time thereafter that a generative artificial intelligence system\\nor service, or a substantial modification to a generative artificial intelligence system or service, released on or\\nafter January 1, 2022, is made publicly available to Californians for use, regardless of whether the terms of that\\nuse include compensation, the developer of the system or service shall post on the developer’s internet website\\ndocumentation regarding the data used by the developer to train the generative artificial intelligence system or\\nservice, including, but not be limited to, all of the following:\\n\\n(a) A high-level summary of the datasets used in the development of the generative artificial intelligence system\\nor service, including, but not limited to:\\n\\n(1) The sources or owners of the datasets. (2) A description of how the datasets further the intended purpose of the artificial intelligence system or\\nservice.']\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use RAG to query our documents and cite the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup required parameters to use AI-verde's OpenAI-compatible API\n",
    "model_name = \"js2/Llama-3.3-70B-Instruct-FP8-Dynamic\"\n",
    "# llm_host = os.environ.get('OPENAI_API_BASE', \"https://llm-api.cyverse.org/v1\")\n",
    "llm_host = os.environ.get('OPENAI_API_BASE', \"https://llm-api.cyverse.ai\")\n",
    "api_key = os.environ.get('OPENAI_API_KEY', 'ADD_YOUR_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directly using langchain ChatOpenAI\n",
    "llm = ChatOpenAI(\n",
    "    model=model_name,\n",
    "    temperature=0.2, \n",
    "    max_tokens=1000,\n",
    "    api_key=api_key,\n",
    "    base_url=llm_host,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello! I'm just a language model, so I don't have feelings or emotions like humans do, but I'm functioning properly and ready to help with any questions or tasks you may have. How about you? How's your day going so far?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 52, 'prompt_tokens': 41, 'total_tokens': 93, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'Llama-3.3-70B-Instruct-FP8-Dynamic', 'system_fingerprint': None, 'id': 'chatcmpl-1cfe7781a83a47fca8b96f98abc23e4f', 'finish_reason': 'stop', 'logprobs': None}, id='run-f8d30082-7084-4955-942b-c741237f2132-0', usage_metadata={'input_tokens': 41, 'output_tokens': 52, 'total_tokens': 93, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Hello, how are you?\") # validate we can talk with the LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up questioning for LLM\n",
    "\n",
    "context = \"\\n\\n\".join(retrieved_chunks)\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You are a legal assistant. Use the following context to answer the user's question. \n",
    "Always cite your sources in brackets like [contract1.pdf chunk 3].\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "User question:\n",
    "{query}\n",
    "\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the case of data reusability, the regulations require developers to provide documentation regarding the data used to train the generative artificial intelligence system or service, including information about the datasets, such as their sources, owners, and intended purpose [CA-AB2013.pdf chunk 6]. Additionally, developers must disclose whether the datasets include any data protected by copyright, trademark, or patent, or whether the datasets are entirely in the public domain [CA-AB2013.pdf chunk 5]. \n",
      "\n",
      "However, the provided context does not specifically address the concept of data reusability. It does mention that developers must post documentation regarding the data used to train the generative artificial intelligence system or service, including information about the datasets, but it does not explicitly discuss what happens when data is reused [WA-1168.pdf chunk 3]. \n",
      "\n",
      "It is worth noting that the European Union's regulations on artificial intelligence emphasize the importance of data governance and management practices, including the assessment of the availability, quantity, and suitability of the data sets, as well as the identification of relevant data gaps or shortcomings [OJ_L_202401689_EN_TXT.pdf chunk 198]. \n",
      "\n",
      "In the context of California's regulations, developers are required to post documentation regarding the data used to train the generative artificial intelligence system or service, but there is an exception for systems or services whose sole purpose is to help ensure security and integrity, or for the operation of aircraft in the national airspace [CA-AB2013.pdf chunk 6]. \n",
      "\n",
      "Overall, while the regulations do not specifically address data reusability, they emphasize the importance of transparency and documentation regarding the data used to train generative artificial intelligence systems or services [CA-AB2013.pdf chunk 3].\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke([prompt])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'completion_tokens': 353,\n",
       "  'prompt_tokens': 2143,\n",
       "  'total_tokens': 2496,\n",
       "  'completion_tokens_details': None,\n",
       "  'prompt_tokens_details': None},\n",
       " 'model_name': 'Llama-3.3-70B-Instruct-FP8-Dynamic',\n",
       " 'system_fingerprint': None,\n",
       " 'id': 'chatcmpl-c82cd3890596453ebb216c667836f27f',\n",
       " 'finish_reason': 'stop',\n",
       " 'logprobs': None}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='In the case of data reusability, the regulations require developers to provide documentation regarding the data used to train the generative artificial intelligence system or service, including information about the datasets, such as their sources, intended purpose, and any modifications made to them [CA-AB2013.pdf chunk 6]. Additionally, developers must disclose whether the datasets include personal information, aggregate consumer information, or protected data, and whether the datasets are entirely in the public domain [WA-1168.pdf chunk 3]. \\n\\nHowever, the provided context does not explicitly address data reusability. Nevertheless, according to Article 10 of the EU regulation [OJ_L_202401689_EN_TXT.pdf chunk 198], high-risk AI systems that make use of techniques involving the training of AI models with data shall be developed on the basis of training, validation, and testing data sets that meet specific quality criteria. These criteria include data governance and management practices, such as data collection processes, data preparation, and assessment of data availability and suitability. \\n\\nIt can be inferred that in the case of data reusability, developers should ensure that the reused data meets these quality criteria and follows the required data governance and management practices to prevent possible biases and ensure compliance with the regulations [OJ_L_202401689_EN_TXT.pdf chunk 198].' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 268, 'prompt_tokens': 2143, 'total_tokens': 2411, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'Llama-3.3-70B-Instruct-FP8-Dynamic', 'system_fingerprint': None, 'id': 'chatcmpl-4de3570251974108b8f329f3102f26ed', 'finish_reason': 'stop', 'logprobs': None} id='run-d6a05534-2bad-45ef-8159-4eb07ecab748-0' usage_metadata={'input_tokens': 2143, 'output_tokens': 268, 'total_tokens': 2411, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "response_text = str(response)\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_citations(output):\n",
    "    pattern = r'\\[(.+?) chunk (\\d+)\\]'\n",
    "    matches = re.findall(pattern, output)\n",
    "    return [{\"source\": src, \"chunk_index\": int(idx)} for src, idx in matches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Source: CA-AB2013.pdf, Chunk: 6\n",
      "  ↪ Open: /pdf_viewer/CA-AB2013.pdf#chunk=6\n",
      "- Source: WA-1168.pdf, Chunk: 3\n",
      "  ↪ Open: /pdf_viewer/WA-1168.pdf#chunk=3\n",
      "- Source: OJ_L_202401689_EN_TXT.pdf, Chunk: 198\n",
      "  ↪ Open: /pdf_viewer/OJ_L_202401689_EN_TXT.pdf#chunk=198\n",
      "- Source: OJ_L_202401689_EN_TXT.pdf, Chunk: 198\n",
      "  ↪ Open: /pdf_viewer/OJ_L_202401689_EN_TXT.pdf#chunk=198\n"
     ]
    }
   ],
   "source": [
    "citations = extract_citations(response_text)\n",
    "\n",
    "for c in citations:\n",
    "    print(f\"- Source: {c['source']}, Chunk: {c['chunk_index']}\")\n",
    "    # Optional: link to PDF viewer\n",
    "    path = f\"/pdf_viewer/{c['source']}#chunk={c['chunk_index']}\"\n",
    "    print(f\"  ↪ Open: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChromaDB -\tFast, persistent, vector store with easy metadata filters\n",
    "\n",
    "Embeddings - Semantic similarity for retrieval\n",
    "\n",
    "Chunking w/ overlap - Keeps context and flow between chunks\n",
    "\n",
    "Source Highlighting\t- Adds transparency and legal traceability\n",
    "\n",
    "RAG Prompting - Combines knowledge + LLM fluency"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
