{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert PDFs to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'%%python' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# %pip install PyPDF2\n",
    "# %pip install openpyxl==3.0.10\n",
    "# %pip install pdfplumber\n",
    "# %pip install tika\n",
    "# %pip install PyMuPDF pandas\n",
    "# %pip install pdf2image pytesseract pillow\n",
    "# %pip install nltk\n",
    "# %pip install spacy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lwert\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Core\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import re #regular expression\n",
    "\n",
    "\n",
    "#OCR\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "# Tokenizers\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.max_length = 3500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocr_on_pdfs(pdf_dir):\n",
    "    #identify the dataset\n",
    "    pdf_data = []\n",
    "\n",
    "    # Loop through all PDF files in the directory\n",
    "    for filename in os.listdir(pdf_dir):\n",
    "        if filename.endswith('.pdf'):\n",
    "            file_path = os.path.join(pdf_dir, filename)\n",
    "            print(\"working on:\", file_path)\n",
    "\n",
    "            try:\n",
    "\n",
    "                #convert file to image\n",
    "                images = convert_from_path(file_path, poppler_path=r\"C:\\poppler-24.08.0\\Library\\bin\")\n",
    "                # Extract the text from all pages\n",
    "                text = []\n",
    "                for img in images:\n",
    "                    text.append(pytesseract.image_to_string(img))\n",
    "\n",
    "                    # Get PDF metadata\n",
    "                    # Open the PDF file\n",
    "                doc = fitz.open(file_path)\n",
    "                metadata = doc.metadata\n",
    "                page_count = doc.page_count\n",
    "\n",
    "\n",
    "                # Create a dictionary to convert to dataframe\n",
    "                pdf_data.append({\n",
    "                    'filename': filename,\n",
    "                    'text': text,\n",
    "                    'page_count': page_count,\n",
    "                    'creator': metadata.get('creator'),\n",
    "                    'producer': metadata.get('producer'),\n",
    "                    'subject': metadata.get('subject'),\n",
    "                    'keywords': metadata.get('keywords'),\n",
    "                    'title': metadata.get('title'),\n",
    "                    'creation_date': metadata.get('creationDate'),\n",
    "                })\n",
    "\n",
    "                doc.close()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"OCR error on {filename}: {e}\")\n",
    "\n",
    "    return pdf_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on: pdf_data\\ARG-1472-D-2023.pdf\n",
      "working on: pdf_data\\CA-2885.pdf\n",
      "working on: pdf_data\\CA-AB2013.pdf\n",
      "working on: pdf_data\\CA-AJR6.pdf\n",
      "working on: pdf_data\\CA-SB1047.pdf\n",
      "working on: pdf_data\\CA-SB942.pdf\n",
      "working on: pdf_data\\CO-CAIA.pdf\n",
      "working on: pdf_data\\DE-H333.pdf\n",
      "working on: pdf_data\\EU-2024-1689.pdf\n",
      "OCR error on EU-2024-1689.pdf: Unable to get page count.\n",
      "Syntax Error: Couldn't find trailer dictionary\n",
      "Syntax Error: Couldn't find trailer dictionary\n",
      "Syntax Error: Couldn't read xref table\n",
      "\n",
      "working on: pdf_data\\FL-S1680.pdf\n",
      "working on: pdf_data\\IL-H4705.pdf\n",
      "working on: pdf_data\\IL-H4836.pdf\n",
      "working on: pdf_data\\IL-H4844.pdf\n",
      "working on: pdf_data\\IN-AI_task_Force.pdf\n",
      "working on: pdf_data\\MD-818.pdf\n",
      "working on: pdf_data\\MEX.pdf\n",
      "working on: pdf_data\\NH_H1688.pdf\n",
      "working on: pdf_data\\NJ-S3357.pdf\n",
      "working on: pdf_data\\NY-A8129.pdf\n",
      "working on: pdf_data\\OJ_L_202401689_EN_TXT.pdf\n",
      "working on: pdf_data\\OR-H4153.pdf\n",
      "working on: pdf_data\\PA-H49.pdf\n",
      "working on: pdf_data\\PA-HR170.pdf\n",
      "working on: pdf_data\\RI-S117.pdf\n",
      "working on: pdf_data\\TN-H2325.pdf\n",
      "working on: pdf_data\\UK.pdf\n",
      "working on: pdf_data\\UT-S149.pdf\n",
      "working on: pdf_data\\UT-SB0149.pdf\n",
      "working on: pdf_data\\VA-H747.pdf\n",
      "working on: pdf_data\\VA-S487.pdf\n",
      "working on: pdf_data\\VT-H710.pdf\n",
      "working on: pdf_data\\WA-1168.pdf\n",
      "working on: pdf_data\\WA-1170.pdf\n",
      "working on: pdf_data\\WA-5838-S2.PL.pdf\n",
      "working on: pdf_data\\WI-A664.pdf\n",
      "working on: pdf_data\\WV-H5690.pdf\n"
     ]
    }
   ],
   "source": [
    "#pull pdf data and run through ocr\n",
    "pdf_dir = \"pdf_data\"\n",
    "pdfs = ocr_on_pdfs(pdf_dir)\n",
    "\n",
    "# Save to JSON\n",
    "with open('legal_pdf_data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(pdfs, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Convert returned dictionary to DataFrame and save\n",
    "df = pd.DataFrame(pdfs)\n",
    "# df.to_csv('legal_pdfs_dataset.csv', index=False)\n",
    "df.to_json('legal_pdf_data.json', orient='records', force_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at: https://github.com/lwdozal/Chat-with-your-Research-Articles-LLM-Retrieval-Augmented-Generation/blob/main/Intermediate%20RAG%20(prefilled).ipynb\n",
    "\n",
    "To start creating embeddings for RAG (This might be an easier start for us, and might still hold some buzzword hype)\n",
    "\n",
    "Tokenize sentences into paragraphs ~5 sentences into each chunk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# legal_texts = pd.DataFrame(legal_texts)\n",
    "\n",
    "with open('legal_pdf_data.json', 'r', encoding='utf-8') as f:\n",
    "    pdf_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_legal_clauses(text, legal_clauses):\n",
    "    # set-up splitter to Find the clauses in the text\n",
    "    clause_splitter = re.compile(r'(?=({}))'.format('|'.join(legal_clauses)), re.IGNORECASE)\n",
    "\n",
    "    # Split based on legal clause words\n",
    "    clauses = clause_splitter.split(text)\n",
    "    \n",
    "    # Clean up chunks and stitch words to their content\n",
    "    refined_clauses = []\n",
    "    i = 0\n",
    "    while i < len(clauses):\n",
    "        if clause_splitter.match(clauses[i]):\n",
    "            # Pair clause header with its content\n",
    "            if i + 1 < len(clauses):\n",
    "                refined_clauses.append((clauses[i] + ' ' + clauses[i+1]).strip())\n",
    "                i += 2\n",
    "            else:\n",
    "                refined_clauses.append(clauses[i].strip())\n",
    "                i += 1\n",
    "        else:\n",
    "            if clauses[i].strip():\n",
    "                refined_clauses.append(clauses[i].strip())\n",
    "            i += 1\n",
    "\n",
    "    return refined_clauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_text(text, legal_clauses, para_size=5, overlap=2):\n",
    "    \"\"\"\n",
    "    Convert flat text into overlapping paragraphs of sentences.\n",
    "    - para_size: number of sentences per paragraph\n",
    "    - overlap: number of sentences to overlap between paragraphs\n",
    "    \"\"\"\n",
    "    #Tokenize into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    # split text based on clauses\n",
    "    clauses = split_legal_clauses(text, legal_clauses)\n",
    "\n",
    "    # split text to create overlapping paragraphs\n",
    "    paragraphs = []\n",
    "    i = 0\n",
    "    while i < len(sentences):\n",
    "        para = sentences[i:i+para_size]\n",
    "        if para:\n",
    "            paragraphs.append(' '.join(para))\n",
    "        i += para_size - overlap  # Slide forward by (para_size - overlap)\n",
    "\n",
    "    return paragraphs, sentences, clauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARG-1472-D-2023.pdf\n",
      "Number of words 8162\n",
      "CA-2885.pdf\n",
      "Number of words 26023\n",
      "CA-AB2013.pdf\n",
      "Number of words 7471\n",
      "CA-AJR6.pdf\n",
      "Number of words 5339\n",
      "CA-SB1047.pdf\n",
      "Number of words 54031\n",
      "CA-SB942.pdf\n",
      "Number of words 11133\n",
      "CO-CAIA.pdf\n",
      "Number of words 42107\n",
      "DE-H333.pdf\n",
      "Number of words 6812\n",
      "FL-S1680.pdf\n",
      "Number of words 8586\n",
      "IL-H4705.pdf\n",
      "Number of words 4333\n",
      "IL-H4836.pdf\n",
      "Number of words 6078\n",
      "IL-H4844.pdf\n",
      "Number of words 3030138\n",
      "IN-AI_task_Force.pdf\n",
      "Number of words 18680\n",
      "MD-818.pdf\n",
      "Number of words 35879\n",
      "MEX.pdf\n",
      "Number of words 33008\n",
      "NH_H1688.pdf\n",
      "Number of words 7431\n",
      "NJ-S3357.pdf\n",
      "Number of words 7388\n",
      "NY-A8129.pdf\n",
      "Number of words 16263\n",
      "OJ_L_202401689_EN_TXT.pdf\n",
      "Number of words 597877\n",
      "OR-H4153.pdf\n",
      "Number of words 4712\n",
      "PA-H49.pdf\n",
      "Number of words 4490\n",
      "PA-HR170.pdf\n",
      "Number of words 10663\n",
      "RI-S117.pdf\n",
      "Number of words 8174\n",
      "TN-H2325.pdf\n",
      "Number of words 8748\n",
      "UK.pdf\n",
      "Number of words 212399\n",
      "UT-S149.pdf\n",
      "Number of words 35275\n",
      "UT-SB0149.pdf\n",
      "Number of words 39725\n",
      "VA-H747.pdf\n",
      "Number of words 20134\n",
      "VA-S487.pdf\n",
      "Number of words 1826\n",
      "VT-H710.pdf\n",
      "Number of words 40628\n",
      "WA-1168.pdf\n",
      "Number of words 7027\n",
      "WA-1170.pdf\n",
      "Number of words 8348\n",
      "WA-5838-S2.PL.pdf\n",
      "Number of words 16291\n",
      "WI-A664.pdf\n",
      "Number of words 3995\n",
      "WV-H5690.pdf\n",
      "Number of words 5432\n"
     ]
    }
   ],
   "source": [
    "# pdf_data\n",
    "\n",
    "# Types of legal clause headers\n",
    "LEGAL_CLAUSES = [\n",
    "    r'WHEREAS\\b',\n",
    "    r'NOW, THEREFORE\\b',\n",
    "    r'BE IT RESOLVED\\b',\n",
    "    r'IN WITNESS WHEREOF\\b',\n",
    "    r'THIS AGREEMENT\\b',\n",
    "    r'FOR THE AVOIDANCE OF DOUBT\\b',\n",
    "    r'SUBJECT TO\\b',\n",
    "    r'NOTWITHSTANDING\\b'\n",
    "]\n",
    "\n",
    "#chunk data into paragraphs and sentences for best Summarization and RAG\n",
    "for doc in pdf_data:\n",
    "    print(\"Working on:\", doc[\"filename\"])\n",
    "    print(\"Number of words:\", len(parsed.text))\n",
    "\n",
    "    text = doc['text']\n",
    "    text = \" \".join(text)\n",
    "    joined_text = text.replace(\"\\n\",\" \")\n",
    "    joined_text = joined_text.replace(\"\\n\\n\",\" \")\n",
    "    parsed = nlp(text)\n",
    "    # print(parsed.text)\n",
    "    \n",
    "    # split the text into sentences, identified clauses, and sentences\n",
    "    paragraphs, sentences, clauses = split_text(parsed.text, LEGAL_CLAUSES, para_size=8, overlap=3)\n",
    "\n",
    "    doc['paragraphs'] = paragraphs\n",
    "    doc['sentences'] = sentences\n",
    "    doc['sentences'] = clauses\n",
    "\n",
    "with open('legal_pdf_tokenized.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(pdf_data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
